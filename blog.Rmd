---
title: "Blog"
description: |
  Various non-project thoughts. 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Introducing...the full-stat data scientist{#fullstat}
2022-11-13

I have been thinking about different kinds of data scientists, and I think we need a new term for some: the __full-stat__ data scientist. 

No, that is not a typo. Doubtless you have seen much about _full-stack_ data scientists - here we compare and contrast.

## What is a _full-stack_ data scientist?

A full-stack data scientist is one who can code, do stats, define metrics, etc. and take an idea from concept through pipeline to database design to building a dashboard. 

They have all of the basic skills of a regular data scientist with the added features of strong data engineering and software engineering skills. 

They are called _full-stack_ because they work with the full stack of data science tools. 

Full-stack data scientists own the data science product, from beginning to end. 

They frequently come with computer science degrees, and have taken a few stats courses, 

They will use words like _kubernetes_, _presto_, _spark_, _python_, _sql_, _pandas_, _pymc_, _scikit-learn_, and _kafka_.


## What is a _full-stat_ data scientist?

A _full-stat_ data scientist is a data scientist who can do much of what a full-stack data scientist can, and may be less skilled in the data engineering realm, but more skilled in statistics and quantitative modeling. 

Many come with advanced degrees in statistics, probability, and econometrics. 

They model the business situation using whatever techniques are required: analysis, stochastic modeling, simulation, optimization, game theory, etc. and determine which statistical techniques are required in order to analyze the data based on the model. 

They can adapt the techniques to the problem when needed or develop completely novel approaches when needed. They can evaluate trade-offs between inferential accuracy and run-time requirements of estimation algorithms. 

They are deeply skeptical about data and find anomalies that no one else does and figure out what to do about them. They care deeply about quantifying uncertainty and the impact of this uncertainty on business decisions.

They will use words like _R_, _Markov chain_, _glm_, _utility function_, _DAG_, and _Stan_. 


## Discussion

Both can be incredibly valuable to your organization. Full-stack data scientists are able to get the data from your customer facing systems into data sets suitable for analysis, create dash boards showing changes in metrics, and run A/B experiments. Full-stat data scientists can probe the deepest recesses of these data sets, and rigorously analyze them in ways that yield new insights.

Note that this does not mean a full-stat data scientist will not know sql, nor that a full-stack can't do a Bayesian analysis - just that their expertise, experience and mindset is generally in their own camp.


# A really fun project{#fun_project}
2022-11-20

I have recently been asked what kind of projects I like to work on. You can see some under the 'Projects' menu. Here I describe one form work, but all the names have been changed to protect the innocent.

The project was about resource allocation. It us a very general problem, but as an example consider users who can take many paths through a system, and depending on what they do may need many different resources, like images, files, computations performed, or queries run. Each of these resources may take a while to prepare and if we knew what the user was going to do next, we could get a head start on preparing whatever the user was going to need next - but of course we don't want to do work for nothing and prepare resources they do not want or need.

My first task was to try to figure out some basic algorithms for predicting what actions the user would take next, so that we could see how the scheme could work in simple case, and then enhance it later where needed.

I used a Markov chain model to predict the users next actions, which means that we model the user's next step only as a function of where they currently are in the user journey. Note that this can be extended, and was in several experiments. Then I tested it against some real user data we had. In some cases the predictions were pretty good, and for others, terrible. I tried to relate the difference in performance of the predictions to characteristics of the users and their current state (ie. what actions they had already taken) , with only a little success. I was hoping that there would be some easily identifiable subset that had good predictions, and then we could prepare the resources for just those users and/or actions and not the others.

To try to figure out why some of the predictions were good and other bad, I developed a model that basically looked like being at a casino. Each step in the user journey consisted of a bag of resources and each of these items had a 'cost' : the amount of computation it would take to prepare them. So, we could think of the problem as betting on specific items, and then if they are actually used in the users next step then we 'win' the savings in computation, because then the user does not have to wait for them to complete. But if the user does not follow our prediction, then that computation is wasted, and we lose. 

Around the same time, the team implemented a preliminary version, and we could evaluate its performance. It agreed with the analysis work I had already done. Again, I tried to zero in some scenarios where the performance was good, but again, that proved difficult.

So I went back to modelling. Some further analysis, followed by some hand-waving scaling limits (it wasn't completely rigorous), yielded an interesting and very simple equation that described the relationship between the cost and the profit for various scenarios. In retrospect the  conclusion was obvious: the concept could only work well in situations where the flow through the system was highly linear, with few choices at each stage.

The business conclusion was then to only look for resources that were involved in a large subset of the likely next steps, and to ready only those that had a high probability of being used at a small cost - which unfortunately also limited the profit.

To me, this project was a blast. It had non-trivial statistics, modelling, real-world impact, and ultimately a simple explanation. Of course, it would have been even more fun if the scheme had worked better!

