{
  "articles": [
    {
      "path": "about.html",
      "title": "About me",
      "description": "I have been doing data science since long before data science was a thing. ",
      "author": [],
      "contents": "\nI have applied data science to telecom while at Alcatel/Nokia, BlackBerry, Akamai; fintech while at Ario/Thinking Capital; and\neCommerce while at Shopify. Most\nrecently, I worked at Meta, on improving\nvideo recommendations.\nI also teach an intro to data science course to MBA students at University of Ottawa.\nI used to co-lead Data for\nGood in Ottawa: volunteer data scientists trying to make Ottawa\nbetter by helping local charities. You can see a bit more about how D4G\nOttawa used to work, and largely still does, here.\nIt is a great way to improve your data science skills, meet cool people,\nand help the city.\nBefore Alcatel, in reverse chronological order: I worked at a\nnetworking start-up (Sedona networks); managed the IT dept for the\nLiberal Party of Canada; did IT consulting for ISA Corporation;\ndeveloped software for gas turbine engine test cells at MDS Aero\nSupport; and worked at Lafarge, first as a strategic planning analyst,\nand later also doing IT support & some coding.\nI have a PhD in Probability and Statistics, my thesis was an abstract\nversion of figuring out how big a reordering buffer was needed on a\nrouter - which lead to figuring out some properties of random Mallows\npermutations. A not-so-technical overview of part of it is here.\nMy M.Sc. was about how to statistically estimate the effective\nbandwidth (which is a measure of resource usage) of various telecom\nnetwork traffic streams.\nYou can find me on LinkedIn, Twitter, and now Mastodon.\n\n\n\n",
      "last_modified": "2022-12-25T12:35:39-05:00"
    },
    {
      "path": "blog.html",
      "title": "Blog",
      "description": "Various non-project thoughts. ",
      "author": [],
      "contents": "\nMaxims of Data Science\n2022-12-01\nOne of the great things about teaching data science (or any subject)\nis that it forces you to distill everything down to just the essentials.\nSo here are my maxims of data science.\nAll your data is crap\nUsing just one number is lying\nSpeak so the other person understands\nLive by the 80/20 rule\nIt must be reproducible\nAll your data is crap\nI am not saying that your data is worthless. I am saying that any\nreasonable data set that is not from a text book, or pre-cleansed, has\nglitches in it….and if you haven’t found them you haven’t looked hard\nenough. Plot, compute summary statistics, look at the data, swim in it.\nLet it seep into every pore.\nOne of the first things you do as a data scientist with a new data\nset is find these glitches, for they can ruin your analysis. Some of\nthem may be easy to explain and remedy (eg, using -99 as a code to\nindicate a missing value), while others may only reveal themselves after\na long game of hide an seek. One example of the latter was a dataset\nwhere packet sizes looked to be periodic…eventually it was discovered\nthat a part of the time stamp field was being written to the packet size\nfield.\nWhich leads to the next point - when you finally figure out what\ncaused the glitch, it is your responsibility to communicate it\nappropriately and make sure the same error can’t happen again.\nThere are, of course, times where you can not discover the cause of a\nglitch. You may not even be certain that there is in fact a glitch, but\nsome element in the data sure looks weird. What do you do? You assess\nthe impact of the glitch on any conclusions you reach from the data set.\nIf the glitch does not impact the decision, or the cost of a wrong\ndecision is low, then you can simply document it. On the other hand if\nit does impact what decision you’d make, then it needs further\nexploration.\nUsing just one number is\nlying\nAnything other than the most trivial of problems can not be described\nby just one number. Don’t be afraid to poke the tails of the\ndistribution of your data to see what your best and worst customers are\nexperiencing. Quantify the variability of your results. A new version of\na system that is 5% faster on average may be a terrible thing to deploy\nif half the customers get worse performance, or if the 5% is really 5%\n+/- 30%.\nOf course you have to figure out the best way to explain that to your\nstakeholders….\nSpeak so the other person\nunderstands\nAll your work is for nothing of the recipient (stakeholder) does not\nunderstand the impact of what your saying. It is the data scientists\nresponsibility to learn the business language and discuss the results in\na way the stakeholder understands their importance - otherwise all your\nwork, even the most earth-shattering results, has all been for\nnothing.\nLive by the 80/20 rule\nThe 80/20 rule is the idea that for many things in life, 20% of the\neffort gets you 80% of the value. And when doing data science that is\nhow you should be thinking - how good does the answer need to be?\nOf course it is not necessarily 80/20. There are many examples where\n80% accuracy is not enough (think medical tests, maybe personal\nfinances, planes flying, etc) and for those you will of course do more\nwork to get a better answer.\nBut the point is do not spend $100 of effort for something that is\nonly worth $1 (unless you are doing it for fun, to learn something new,\nor a variety of other reasons that, in fact, increase the value to you\nfrom $1)\nIt must be reproducible\nNothing loses hard-won credibility for a data scientist faster than\nto re-doing an analysis with very similar numbers and get wildly\ndifferent answers - so you then rerun with the original numbers and get\nanswers different from your original analysis. How did that happen? If\nall you can reply with is a shrug, you have lost your audience.\nReproducibility prevents that. Most modern data science environments\n(think R/RStudio/Knitr) allow you to create a document that can rerun\nthe analysis form beginning to end with the click of a button, and the\nfiles that are used can be easily version controlled (think git).\nYour future self will thank you when you need to redo the analysis in\na few months with slightly different data and you don’t have to remember\nevery fiddly step because it is all in your code/document.\nConclusion\nData science, done well, can be a force for good. Done poorly it just\nmakes the world more confused. Keeping these few simple guidelines in\nmind can help you do the former.\nA really fun project\n2022-11-20\nI have recently been asked what kind of projects I like to work on.\nYou can see some under the ‘Projects’ menu. Here I describe one from\nwork, but all the names have been changed to protect the innocent.\nThe project was about resource allocation. It us a very general\nproblem, but as an example consider users who can take many paths\nthrough a system, and depending on what they do may need many different\nresources, like images, files, computations performed, or queries run.\nEach of these resources may take a while to prepare and if we knew what\nthe user was going to do next, we could get a head start on preparing\nwhatever the user was going to need next - but of course we don’t want\nto do work for nothing and prepare resources they do not want or\nneed.\nThe first task was to try to figure out some basic algorithms for\npredicting what actions the user would take next, so that we could see\nhow the scheme could work in simple cases, and then enhance later where\nneeded.\nI used a Markov chain model to predict the users next actions, which\nmeans that we model the user’s next step only as a function of where\nthey currently are in the user journey. Note that this can be extended,\nand was in several experiments. Then I tested it against some real user\ndata we had. In some cases the predictions were pretty good, and for\nothers, terrible. I tried to relate the difference in performance of the\npredictions to characteristics of the users and their current state (ie.\nwhat actions they had already taken) , with only a little success. I was\nhoping that there would be some easily identifiable subset that had good\npredictions, and then we could prepare the resources for just those\nusers and/or actions and not the others.\nTo try to figure out why some of the predictions were good and other\nbad, I developed a model that basically looked like being at a casino.\nEach step in the user journey consisted of a bag of resources and each\nof these items had a ‘cost’ : the amount of computation it would take to\nprepare them. So, we could think of the problem as betting on specific\nitems, and then if they are actually used in the users next step then we\n‘win’ the savings in computation, because then the user does not have to\nwait for them to complete. But if the user does not follow our\nprediction, then that computation is wasted, and we lose.\nAround the same time, the team implemented a preliminary version, and\nwe could evaluate its performance. It agreed with the analysis work I\nhad already done. Again, I tried to zero in some scenarios where the\nperformance was good, but again, that proved difficult.\nSo I went back to modelling. Some further analysis, followed by some\nhand-waving scaling limits (it wasn’t completely rigorous), yielded an\ninteresting and very simple equation that described the relationship\nbetween the cost and the profit for various scenarios. In retrospect the\nconclusion was obvious: the concept could only work well in situations\nwhere the flow through the system was highly linear, with few choices at\neach stage.\nThe business conclusion was then to only look for resources that were\ninvolved in a large subset of the likely next steps, and to ready only\nthose that had a high probability of being used at a small cost - which\nunfortunately also limited the profit.\nTo me, this project was a blast. It had non-trivial statistics,\nmodelling, real-world impact, and ultimately a simple explanation. Of\ncourse, it would have been even more fun if the scheme had worked\nbetter!\nIntroducing…the full-stat data scientist\n2022-11-13\nI have been thinking about different kinds of data scientists, and I\nthink we need a new term for some: the full-stat data\nscientist.\nNo, that is not a typo. Doubtless you have seen much about\nfull-stack data scientists - here we compare and contrast.\nWhat is a\nfull-stack data scientist?\nA full-stack data scientist is one who can code, do stats, define\nmetrics, etc. and take an idea from concept through pipeline to database\ndesign to building a dashboard.\nThey have all of the basic skills of a regular data scientist with\nthe added features of strong data engineering and software engineering\nskills.\nThey are called full-stack because they work with the full\nstack of data science tools.\nFull-stack data scientists own the data science product, from\nbeginning to end.\nThey frequently come with computer science degrees, and have taken a\nfew stats courses,\nThey will use words like kubernetes, presto,\nspark, python, sql, pandas,\npymc, scikit-learn, and kafka.\nWhat is a full-stat\ndata scientist?\nA full-stat data scientist is a data scientist who can do\nmuch of what a full-stack data scientist can, and may be less skilled in\nthe data engineering realm, but more skilled in statistics and\nquantitative modeling.\nMany come with advanced degrees in statistics, probability, and\neconometrics.\nThey model the business situation using whatever techniques are\nrequired: analysis, stochastic modeling, simulation, optimization, game\ntheory, etc. and determine which statistical techniques are required in\norder to analyze the data based on the model.\nThey can adapt the techniques to the problem when needed or develop\ncompletely novel approaches when needed. They can evaluate trade-offs\nbetween inferential accuracy and run-time requirements of estimation\nalgorithms.\nThey are deeply skeptical about data and find anomalies that no one\nelse does and figure out what to do about them. They care deeply about\nquantifying uncertainty and the impact of this uncertainty on business\ndecisions.\nThey will use words like R, Markov chain,\nglm, utility function, DAG, and\nStan.\nDiscussion\nBoth can be incredibly valuable to your organization. Full-stack data\nscientists are able to get the data from your customer facing systems\ninto data sets suitable for analysis, create dash boards showing changes\nin metrics, and run A/B experiments. Full-stat data scientists can probe\nthe deepest recesses of these data sets, and rigorously analyze them in\nways that yield new insights.\nNote that this does not mean a full-stat data scientist will not know\nsql, nor that a full-stack can’t do a Bayesian analysis - just that\ntheir expertise, experience and mindset is generally in their own\ncamp.\n\n\n\n",
      "last_modified": "2022-12-28T08:21:18-05:00"
    },
    {
      "path": "index.html",
      "title": "Peter's Stuff",
      "description": "Welcome to my website\n",
      "author": [],
      "contents": "\nTo see more about me look at the stuff in the menus (upper\nright).\nOtherwise, please enjoy Isaac & Kurt contemplating the monolith:\n\n\n\n\n\n\n",
      "last_modified": "2022-12-25T12:35:40-05:00"
    },
    {
      "path": "projects.html",
      "title": "Projects",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-12-25T12:35:40-05:00"
    },
    {
      "path": "snippets.html",
      "title": "Snippets",
      "description": "Little bits of code. ",
      "author": [],
      "contents": "\nThis is a place where I will put little bits of useful code with\nexamples…for you to learn from, and for me to remember!\nUnfortunately, I have lost the references to most of them, so to\nthose of you out there who figured these out first, thanks &\nsorry!\nPipelining a wildcard in a filter\n2022-12-25\nSo here is the scenario. I have a data frame like so:\n\n\ndf <- tribble(\n  ~doc, ~val,\n  'B',1,\n  'B',2,\n  'BC',1,\n  'BC',4,\n  'D',5\n)\n\ndf\n\n\n# A tibble: 5 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n3 BC        1\n4 BC        4\n5 D         5\n\nand I want to do a filter in a long sequence of code. But I want to\nbe able to find all rows if the user enters ‘ALL’, or just the matching\nrows if they enter something else. So ‘ALL’ acts like a wildcard.\nFor example, if the user enters (say into a variable\ndocname) B, I want the pipelline to return the first\ntwo rows. But if they enter ALL, I want all rows returned.\nHere is how to do it.\n\n\ndocname<-'B'\ndf %>% {if(docname!='ALL') filter(., doc == docname) else .} \n\n\n# A tibble: 2 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n\n\n\ndocname<-'BC'\ndf %>% {if(docname!='ALL') filter(., doc == docname) else .} \n\n\n# A tibble: 2 × 2\n  doc     val\n  <chr> <dbl>\n1 BC        1\n2 BC        4\n\n\n\ndocname<-'ALL'\ndf %>% {if(docname!='ALL') filter(., doc == docname) else .} \n\n\n# A tibble: 5 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n3 BC        1\n4 BC        4\n5 D         5\n\nLabelling in latex\n2022-12-25\nWhen writing a mathy document, it can be helpful to explain what each\npart of an equation means. underbrace and text are\nyour friends.\nHere is an example\n\\[y=\\underbrace{3 \\alpha}_\\text{What alpha\nmeans}\\times \\underbrace{f\\left( x^3\\right)}_\\text{why x\ncubed?}\\] The latex is:\ny=\\underbrace{3 \\alpha}_\\text{What alpha means}\\times \\underbrace{f\\left( x^3\\right)}_\\text{why x cubed?}\n\n\n\n",
      "last_modified": "2022-12-25T12:35:42-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
