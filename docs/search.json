{
  "articles": [
    {
      "path": "about.html",
      "title": "About me",
      "description": "I have been doing data science since long before data science was a thing. ",
      "author": [],
      "contents": "\nI have applied data science to telecom while at Alcatel/Nokia, BlackBerry, Akamai; fintech while at Ario/Thinking Capital; eCommerce while at Shopify, and at Meta on improving video recommendations. I now work at Viamo, “improving lives via mobile” … ok, ok … “Improving ‘improving lives via mobile’ via data!”\nI also teach an intro to data science course to MBA students at University of Ottawa.\nI used to co-lead Data for Good in Ottawa: volunteer data scientists trying to make Ottawa better by helping local charities. You can see a bit more about how D4G Ottawa used to work, and largely still does, here. It is a great way to improve your data science skills, meet cool people, and help the city.\nBefore Alcatel, in reverse chronological order: I worked at a networking start-up (Sedona networks); managed the IT dept for the Liberal Party of Canada; did IT consulting for ISA Corporation; developed software for gas turbine engine test cells at MDS Aero Support; and worked at Lafarge, first as a strategic planning analyst, and later also doing IT support & some coding.\nI have a PhD in Probability and Statistics, my thesis was an abstract version of figuring out how big a reordering buffer was needed on a router - which lead to figuring out some properties of random Mallows permutations. A not-so-technical overview of part of it is here.\nMy M.Sc. was about how to statistically estimate the effective bandwidth (which is a measure of resource usage) of various telecom network traffic streams.\nYou can find me on LinkedIn, Twitter, and/or Mastodon.\n\n\n\n",
      "last_modified": "2023-03-12T15:34:28-04:00"
    },
    {
      "path": "blog.html",
      "title": "Blog",
      "description": "Various non-project thoughts. ",
      "author": [],
      "contents": "\nMaxims of Data Science\n2022-12-01\nOne of the great things about teaching data science (or any subject)\nis that it forces you to distill everything down to just the essentials.\nSo here are my maxims of data science.\nAll your data is crap\nUsing just one number is lying\nSpeak so the other person understands\nLive by the 80/20 rule\nIt must be reproducible\nAll your data is crap\nI am not saying that your data is worthless. I am saying that any\nreasonable data set that is not from a text book, or pre-cleansed, has\nglitches in it….and if you haven’t found them you haven’t looked hard\nenough. Plot, compute summary statistics, look at the data, swim in it.\nLet it seep into every pore.\nOne of the first things you do as a data scientist with a new data\nset is find these glitches, for they can ruin your analysis. Some of\nthem may be easy to explain and remedy (eg, using -99 as a code to\nindicate a missing value), while others may only reveal themselves after\na long game of hide an seek. One example of the latter was a dataset\nwhere packet sizes looked to be periodic…eventually it was discovered\nthat a part of the time stamp field was being written to the packet size\nfield.\nWhich leads to the next point - when you finally figure out what\ncaused the glitch, it is your responsibility to communicate it\nappropriately and make sure the same error can’t happen again.\nThere are, of course, times where you can not discover the cause of a\nglitch. You may not even be certain that there is in fact a glitch, but\nsome element in the data sure looks weird. What do you do? You assess\nthe impact of the glitch on any conclusions you reach from the data set.\nIf the glitch does not impact the decision, or the cost of a wrong\ndecision is low, then you can simply document it. On the other hand if\nit does impact what decision you’d make, then it needs further\nexploration.\nUsing just one number is\nlying\nAnything other than the most trivial of problems can not be described\nby just one number. Don’t be afraid to poke the tails of the\ndistribution of your data to see what your best and worst customers are\nexperiencing. Quantify the variability of your results. A new version of\na system that is 5% faster on average may be a terrible thing to deploy\nif half the customers get worse performance, or if the 5% is really 5%\n+/- 30%.\nOf course you have to figure out the best way to explain that to your\nstakeholders….\nSpeak so the other person\nunderstands\nAll your work is for nothing of the recipient (stakeholder) does not\nunderstand the impact of what your saying. It is the data scientists\nresponsibility to learn the business language and discuss the results in\na way the stakeholder understands their importance - otherwise all your\nwork, even the most earth-shattering results, has all been for\nnothing.\nLive by the 80/20 rule\nThe 80/20 rule is the idea that for many things in life, 20% of the\neffort gets you 80% of the value. And when doing data science that is\nhow you should be thinking - how good does the answer need to be?\nOf course it is not necessarily 80/20. There are many examples where\n80% accuracy is not enough (think medical tests, maybe personal\nfinances, planes flying, etc) and for those you will of course do more\nwork to get a better answer.\nBut the point is do not spend $100 of effort for something that is\nonly worth $1 (unless you are doing it for fun, to learn something new,\nor a variety of other reasons that, in fact, increase the value to you\nfrom $1)\nIt must be reproducible\nNothing loses hard-won credibility for a data scientist faster than\nto re-doing an analysis with very similar numbers and get wildly\ndifferent answers - so you then rerun with the original numbers and get\nanswers different from your original analysis. How did that happen? If\nall you can reply with is a shrug, you have lost your audience.\nReproducibility prevents that. Most modern data science environments\n(think R/RStudio/Knitr) allow you to create a document that can rerun\nthe analysis form beginning to end with the click of a button, and the\nfiles that are used can be easily version controlled (think git).\nYour future self will thank you when you need to redo the analysis in\na few months with slightly different data and you don’t have to remember\nevery fiddly step because it is all in your code/document.\nConclusion\nData science, done well, can be a force for good. Done poorly it just\nmakes the world more confused. Keeping these few simple guidelines in\nmind can help you do the former.\nA really fun project\n2022-11-20\nI have recently been asked what kind of projects I like to work on.\nYou can see some under the ‘Projects’ menu. Here I describe one from\nwork, but all the names have been changed to protect the innocent.\nThe project was about resource allocation. It us a very general\nproblem, but as an example consider users who can take many paths\nthrough a system, and depending on what they do may need many different\nresources, like images, files, computations performed, or queries run.\nEach of these resources may take a while to prepare and if we knew what\nthe user was going to do next, we could get a head start on preparing\nwhatever the user was going to need next - but of course we don’t want\nto do work for nothing and prepare resources they do not want or\nneed.\nThe first task was to try to figure out some basic algorithms for\npredicting what actions the user would take next, so that we could see\nhow the scheme could work in simple cases, and then enhance later where\nneeded.\nI used a Markov chain model to predict the users next actions, which\nmeans that we model the user’s next step only as a function of where\nthey currently are in the user journey. Note that this can be extended,\nand was in several experiments. Then I tested it against some real user\ndata we had. In some cases the predictions were pretty good, and for\nothers, terrible. I tried to relate the difference in performance of the\npredictions to characteristics of the users and their current state (ie.\nwhat actions they had already taken) , with only a little success. I was\nhoping that there would be some easily identifiable subset that had good\npredictions, and then we could prepare the resources for just those\nusers and/or actions and not the others.\nTo try to figure out why some of the predictions were good and other\nbad, I developed a model that basically looked like being at a casino.\nEach step in the user journey consisted of a bag of resources and each\nof these items had a ‘cost’ : the amount of computation it would take to\nprepare them. So, we could think of the problem as betting on specific\nitems, and then if they are actually used in the users next step then we\n‘win’ the savings in computation, because then the user does not have to\nwait for them to complete. But if the user does not follow our\nprediction, then that computation is wasted, and we lose.\nAround the same time, the team implemented a preliminary version, and\nwe could evaluate its performance. It agreed with the analysis work I\nhad already done. Again, I tried to zero in some scenarios where the\nperformance was good, but again, that proved difficult.\nSo I went back to modelling. Some further analysis, followed by some\nhand-waving scaling limits (it wasn’t completely rigorous), yielded an\ninteresting and very simple equation that described the relationship\nbetween the cost and the profit for various scenarios. In retrospect the\nconclusion was obvious: the concept could only work well in situations\nwhere the flow through the system was highly linear, with few choices at\neach stage.\nThe business conclusion was then to only look for resources that were\ninvolved in a large subset of the likely next steps, and to ready only\nthose that had a high probability of being used at a small cost - which\nunfortunately also limited the profit.\nTo me, this project was a blast. It had non-trivial statistics,\nmodelling, real-world impact, and ultimately a simple explanation. Of\ncourse, it would have been even more fun if the scheme had worked\nbetter!\nIntroducing…the full-stat data scientist\n2022-11-13\nI have been thinking about different kinds of data scientists, and I\nthink we need a new term for some: the full-stat data\nscientist.\nNo, that is not a typo. Doubtless you have seen much about\nfull-stack data scientists - here we compare and contrast.\nWhat is a\nfull-stack data scientist?\nA full-stack data scientist is one who can code, do stats, define\nmetrics, etc. and take an idea from concept through pipeline to database\ndesign to building a dashboard.\nThey have all of the basic skills of a regular data scientist with\nthe added features of strong data engineering and software engineering\nskills.\nThey are called full-stack because they work with the full\nstack of data science tools.\nFull-stack data scientists own the data science product, from\nbeginning to end.\nThey frequently come with computer science degrees, and have taken a\nfew stats courses,\nThey will use words like kubernetes, presto,\nspark, python, sql, pandas,\npymc, scikit-learn, and kafka.\nWhat is a full-stat\ndata scientist?\nA full-stat data scientist is a data scientist who can do\nmuch of what a full-stack data scientist can, and may be less skilled in\nthe data engineering realm, but more skilled in statistics and\nquantitative modeling.\nMany come with advanced degrees in statistics, probability, and\neconometrics.\nThey model the business situation using whatever techniques are\nrequired: analysis, stochastic modeling, simulation, optimization, game\ntheory, etc. and determine which statistical techniques are required in\norder to analyze the data based on the model.\nThey can adapt the techniques to the problem when needed or develop\ncompletely novel approaches when needed. They can evaluate trade-offs\nbetween inferential accuracy and run-time requirements of estimation\nalgorithms.\nThey are deeply skeptical about data and find anomalies that no one\nelse does and figure out what to do about them. They care deeply about\nquantifying uncertainty and the impact of this uncertainty on business\ndecisions.\nThey will use words like R, Markov chain,\nglm, utility function, DAG, and\nStan.\nDiscussion\nBoth can be incredibly valuable to your organization. Full-stack data\nscientists are able to get the data from your customer facing systems\ninto data sets suitable for analysis, create dash boards showing changes\nin metrics, and run A/B experiments. Full-stat data scientists can probe\nthe deepest recesses of these data sets, and rigorously analyze them in\nways that yield new insights.\nNote that this does not mean a full-stat data scientist will not know\nsql, nor that a full-stack can’t do a Bayesian analysis - just that\ntheir expertise, experience and mindset is generally in their own\ncamp.\n\n\n\n",
      "last_modified": "2022-12-28T08:21:18-05:00"
    },
    {
      "path": "index.html",
      "title": "Peter's Stuff",
      "description": "Welcome to my website\n",
      "author": [],
      "contents": "\nTo see more about me look at the stuff in the menus (upper\nright).\nOtherwise, please enjoy Isaac & Kurt contemplating the monolith:\n\n\n\n\n\n\n",
      "last_modified": "2022-12-25T12:35:40-05:00"
    },
    {
      "path": "jobblog.html",
      "title": "Job Blog",
      "description": "Thoughts about careers. ",
      "author": [],
      "contents": "\nMany years ago I wrote a post about getting a job in data science.\nThis one is about technical interviews for ML eng type positions.\nPlease keep in mind that the following are my opinions only, and do not necessarily represent those of any organization I am or have been associated with. In other words, ymmv.\nWhat you can expect in a technical interview\nThe interview\nSo, you’ve been selected for a technical interview. Depending on the company, these can range from stressful to “OMG I’m dying”. We know that and so we (the interviewers) will try to help you along the way. You’ve gotten this far in the process and we want you to succeed. We want hire you and get back to making our customers happy.\nSo here is the one weird trick to doing well: listen to what the interviewers are saying.\nThe information we provide will help you frame your answer. If our question is unclear - ask questions. We want to see how you think, how you deal with ambiguity (much as you will if we hire you!)\nIf we tell you “don’t worry about X in this problem” and then you spend five minutes talking about X, we think either you are a poor communicator, or you do not understand what X is. Either way, that is not good.\n\n\nBe prepared to discuss a project from you resume in excruciating depth.\nWe can see from your resume the breadth of your experience - what is harder to ascertain is your depth. We want (need) people who can go deep, beyond the basics. You don’t need to be an expert in everything, but we need to see that you can go deep in something.\nFrequently the interviewer will ask you to speak about a project you have worked on. This is your opportunity to describe one that you can go deep on. Be sure to explain the business context, the results achieved, and your role - not just the technology. We want to know what impact it had on the business and how you made that happen.\n\n\nIf the position involves coding, you’ve got to be able to code.\nWe will typically have a small problem that should be solvable in a few minutes, and we want to see how you approach it. Talk us through your thinking and listen to the feedback we give. Get clarification when needed. We know that in the real world you will be googling things, that the IDE will auto-complete the function name, etc. It is fine in these exercises to say “I don’t remember the exact syntax of the function, but it takes a date-timestamp and returns the month as a character string - let us assume it is called ‘date_timestamp_to_month’ and go from there”. We want to se that you know such a thing exists, and what it must do. Of course, we may then say, “ok, please write your ‘date_timestamp_to_month’ function”.\nYou should also talk about the implications of the code you have just written - both technical and business wise. For example, if you re summarizing a data table, what are the technical issues you may encounter - how will your code perform with a million rows? A billion? What if there are missing values? And what are the business implications of these missing values? How will you handle them if they do matter, and if they don’t?\n\n\nAs I said, in the real world you will be looking up things all the time - but during the interview is not the time. If we ask you a question and you stop to think, but we see that you are reading something, and then speak the perfect formal definition of what we just asked you - we strongly suspect you have looked something up. And if you don’t tell us that, you appear sneaky and our trust in you drops.\nKnow about us. Read our web site. Adapt your answers to what our business problems are. You can (and should) always ask - but better to have some context on which to base the conversation. For example when talking to a company that builds IVR systems, don’t blather on about designing a GUI.\nA few words about your resume\nYour resume is the first thing, and perhaps the last thing, we see about you so make it count.\nReview it before you send it. If your resume has typos on it, it looks like you didn’t care enough to proof read it, or are sloppy. Spell checkers are free, and asking a friend to review it costs nothing other than time, or maybe a cup of coffee.\nAlso, make sure you spell the names of things properly. If you have gone to “John Hopkins University”, your resume ends up in the trash bin. If you went there you should know how to spell its name, which is “Johns Hopkins University”.\n\nDon’t stretch the truth (it will come out). Honesty on your resume is important. If you’ve done a few courses on Coursera where the professor was from Johns Hopkins, you did not attend Johns Hopkins. If you completed a few master’s level courses, you do not have a masters degree – you simply did some master’s level courses.\nMake it easy to see what you’ve done and what you specialize in. Make it accurate and easy to understand. Skip the grades and references (you can offer these after you get an interview)\nWhat will really make you stand out is interesting projects…especially ones you have done outside of school/work because most of your work projects are likely covered by an NDA and so you can’t talk about them in any depth.\nBut, if your project is on the mnist data set, unless your algorithm can beat the best in the world I’m not interested, I want to see something that you were interested in and dove deep into.\n\nIf it is on your resume, you should know it. Please don’t say on your resume the you are intermediate in C and then when we ask you a C related question reply that it is the third letter in the alphabet. If it is on your resume it is fair game, and if we ask you about something that is there and you don’t know about it, we question the accuracy of the rest of your resume.\nConclusion\nI repeat what I said earlier - we want to hire you. The above are just some pointers to help make that process easier for both of us.\n\n\n\n",
      "last_modified": "2023-07-24T11:27:24-04:00"
    },
    {
      "path": "projects.html",
      "title": "Projects",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-01-05T11:33:48-05:00"
    },
    {
      "path": "snippets.html",
      "title": "Snippets",
      "description": "Little bits of code. ",
      "author": [],
      "contents": "\nThis is a place where I will put little bits of useful code with\nexamples…for you to learn from, and for me to remember!\nUnfortunately, I have lost the references to most of them, so to\nthose of you out there who figured these out first, thanks &\nsorry!\nPipelining a wildcard in a filter\n2022-12-25\nSo here is the scenario. I have a data frame like so:\n\n\ndf <- tribble(\n  ~doc, ~val,\n  'B',1,\n  'B',2,\n  'BC',1,\n  'BC',4,\n  'D',5\n)\n\ndf\n\n\n# A tibble: 5 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n3 BC        1\n4 BC        4\n5 D         5\n\nand I want to do a filter in a long sequence of code. But I want to\nbe able to find all rows if the user enters ‘ALL’, or just the matching\nrows if they enter something else. So ‘ALL’ acts like a wildcard.\nFor example, if the user enters (say into a variable\ndocname) B, I want the pipeline to return the first\ntwo rows. But if they enter ALL, I want all rows returned.\nHere is how to do it.\n\n\ndocname<-'B'\ndf %>% {if(docname!='ALL') filter(., doc == docname) else .} \n\n\n# A tibble: 2 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n\n\n\ndocname<-'BC'\ndf %>% {if(docname!='ALL') filter(., doc == docname) else .} \n\n\n# A tibble: 2 × 2\n  doc     val\n  <chr> <dbl>\n1 BC        1\n2 BC        4\n\n\n\ndocname<-'ALL'\ndf %>% {if(docname!='ALL') filter(., doc == docname) else .} \n\n\n# A tibble: 5 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n3 BC        1\n4 BC        4\n5 D         5\n\nFurther discussions with Chris have lead me to conclude\nthat the best way to do this is to use anonymous functions like the\nfollowing, rather than the curly bracket trick:\n\n\ndf |> (\\(.) if(docname!='ALL') filter(., doc == docname) else .)()\n\n\n# A tibble: 5 × 2\n  doc     val\n  <chr> <dbl>\n1 B         1\n2 B         2\n3 BC        1\n4 BC        4\n5 D         5\n\nThis way composes well with other steps in a pipeline and does not\nrequire using helper functions that litter the code. And it works with\nboth %>% and |>.\nLabelling in latex\n2022-12-25\nWhen writing a mathy document, it can be helpful to explain what each\npart of an equation means. underbrace and text are\nyour friends.\nHere is an example\n\\[y=\\underbrace{3 \\alpha}_\\text{What alpha\nmeans}\\times \\underbrace{f\\left( x^3\\right)}_\\text{why x\ncubed?}\\] The latex is:\ny=\\underbrace{3 \\alpha}_\\text{What alpha means}\\times \\underbrace{f\\left( x^3\\right)}_\\text{why x cubed?}\nFor a slightly better looking version with just a bit more\ncomplication:\n\\[ y = \\underbrace{3 \\alpha}_{\\text{What }\n\\alpha \\text{ means}} \\times \\underbrace{f\\left( x^3\\right)}_{\\text{why\n} x^3 \\text{?}} \\]\nThe latex here is:\ny = \\underbrace{3 \\alpha}_{\\text{What } \\alpha \\text{ means}} \\times \\underbrace{f\\left( x^3\\right)}_{\\text{why } x^3 \\text{?}}\n\n\n\n",
      "last_modified": "2022-12-30T08:54:05-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
